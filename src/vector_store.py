from langchain.embeddings import HuggingFaceBgeEmbeddings
from langchain_community.document_loaders import TextLoader
from langchain_text_splitters import CharacterTextSplitter
from langchain_community.vectorstores import FAISS
from langchain_core.vectorstores import VectorStore
from langchain_community.document_loaders import DirectoryLoader
from langchain.schema import Document
from src.config import device
from tqdm import tqdm
import itertools
import logging
import warnings
import time
from typing import Iterable
import json

class RAGVectorStore:
    '''Vector store Wrapper'''
    
    def __init__(self, data_dir: str, store_type="FAISS", chunk_size: int = 1000, chunk_overlap: int = 30):

        self.data_dir = data_dir
        self.store_type = store_type

        #document loader
        self.loader = DirectoryLoader(self.data_dir, glob="**/*.txt", loader_cls=TextLoader, use_multithreading=True, show_progress=True)
        
        #text splitter
        self.text_splitter = CharacterTextSplitter(separator="", chunk_size=chunk_size, chunk_overlap=chunk_overlap)

        self.embedding_model = HuggingFaceBgeEmbeddings(
            model_name="BAAI/bge-small-en-v1.5",
            model_kwargs={
                'device': device
            }
        )
        
    def create_db(self, db_name : str, verbose : bool = False) -> object:
        """Returns fully initialized vector database with vector embeddings generated by specified embedding model (self.embedding_model) 
        and similiarity search native to the vector store type (self.store_type).

        Args:
            verbose (bool, optional): Explicitly outlines the steps of creating and returning the database. Defaults to False.

        Returns:
            object: The vector database.
        """

        docs = list(itertools.chain.from_iterable(self.loader.load()))

        if verbose:
            
            print("Documents Loaded")

        #suppress warnings for this next part
        warnings.filterwarnings("ignore")

        documents_split = self.text_splitter.split_documents(docs)

        if verbose:
        
            print("Documents Split")
            print("Beginning Embeddings")

        db = None
        
        #begin embedding
        if self.store_type == "FAISS":

            with tqdm(total=len(documents_split), desc="Ingesting documents") as pbar:

                for d in documents_split:

                    if db:

                        db.add_documents([d])

                    else:

                        db = FAISS.from_documents([d], self.embedding_model)

                    pbar.update(1)

        db.save_local("vector_stores/{fn}".format(fn=db_name))
        
        return db

    def save_db(self, vector_store : VectorStore, db_name : str) -> None:

        vector_store.save_local("vector_stores/{fn}".format(fn=db_name))

    def load_db(self, db_name : str) -> VectorStore:

        if self.store_type == "FAISS":
       
            return FAISS.load_local("vector_stores/{fn}".format(fn=db_name), embeddings=self.embedding_model, allow_dangerous_deserialization=True)
      
        return None
    
    def load_docs(self) -> None:
        
        return list(itertools.chain.from_iterable(self.loader.load()))

    def chunk_to_jsonl(self, jsonl_path: str = None) -> None:
        """Chunk iterable of Documents and write to JSONL file.

        Args:
            jsonl_path (str, optional): JSONL file path. Defaults to None.
        """
        
        docs = self.load_docs()
        
        #suppress warnings for text splitting
        warnings.filterwarnings("ignore")

        documents_split = self.text_splitter.split_documents(docs)
        
        print("Documents Split")
        
        if not jsonl_path:
            jsonl_path = "chunked_data" + "_" + time.strftime("%Y%m%d-%H%M%S")
        
        with open(jsonl_path, 'w') as jsonl_file:
            for doc in documents_split:
                jsonl_file.write(doc.json() + '\n')
    
    def load_docs_from_jsonl(self, jsonl_path: str) -> None:
        """Load Iterable of Documents from a JSONL file.

        Args:
            jsonl_path (str): JSONL file path.

        Returns:
            Iterable[Documents]: Iterable of Documents.
        """
        
        docs = []
        
        with open(jsonl_path, 'r') as jsonl_file:
            for line in jsonl_file:
                data = json.loads(line)
                obj = Document(**data)
                docs.append(obj)
                
        return docs