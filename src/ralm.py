from typing import Any

class RALM():
    '''Generic class wrapping retrieval augmented langauge models'''

    def __init__(self, vector_db : Any): 
        self.vector_db = vector_db
    
    def retrieve_context(self, question : str) -> list[str]:
        '''# TODO: write retrieval process here'''
        relevant_prompts = self.vector_db.similarity_search(question)

        list_prompts = []
        for i in range(len(relevant_prompts)):
            list_prompts.append(relevant_prompts[i].page_content)

        return list_prompts
        
    def generate_prompt(self, question : str) -> str:
        '''Generates a prompt (context, question, specifications), provided the question'''
        context = " , ".join(self.retrieve_context(question))
        prompt = f"<$BEGIN_CONTEXT> {context} <$END_CONTEXT>\n<$BEGIN_QUESTION> {question} <$END_QUESTION>\n"
        prompt += "Given CONTEXT between <$BEGIN_CONTEXT> and <$END_CONTEXT>, respond to the QUESTION between <$BEGIN_QUESTION> and <$END_QUESTION>. If the selected CONTEXT is relevant and informative, provide a detailed answer based on its content. However, if the selected CONTEXT does not offer useful information or is not applicable, simply state 'No answer found'."
        return prompt

    def format_output(self, output : Any) -> str:
        '''Trims or performs other operations on the LM's output to convert to a readable string'''
        pass

    def predict(self, question : str) -> str:
        '''Computes the response of the LM given the appropriate prompt generated by the question'''
        pass